#!/usr/bin/env python3
"""
Syst√®me d'am√©lioration continue des mod√®les ML
Utilise les pr√©dictions sauvegard√©es + r√©sultats r√©els pour r√©-entra√Æner p√©riodiquement
"""

import os
import sys
import json
import pandas as pd
import numpy as np
from pathlib import Path
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Tuple
import joblib
import warnings
warnings.filterwarnings('ignore')

# Ajouter le chemin du projet
sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))

try:
    from sklearn.model_selection import train_test_split
    from sklearn.preprocessing import StandardScaler
    from sklearn.metrics import accuracy_score, classification_report, roc_auc_score
    import xgboost as xgb
except ImportError as e:
    print(f"‚ùå Erreur import sklearn/xgboost: {e}")
    sys.exit(1)

from backend.features import compute_features, EXPECTED_FEATURES
from backend.adaptive_predict import get_symbol_category, MODEL_CONFIGS, create_adaptive_features

# Chemin du dossier MT5 Predictions
MT5_PREDICTIONS_DIR = Path(
    r"C:\Users\USER\AppData\Roaming\MetaQuotes\Terminal\Common\Files\Predictions"
)

class ContinuousLearning:
    """
    Syst√®me d'apprentissage continu pour am√©liorer les mod√®les ML
    """
    
    def __init__(self, min_new_samples: int = 100, retrain_interval_days: int = 7):
        """
        Args:
            min_new_samples: Nombre minimum de nouvelles pr√©dictions avant r√©-entra√Ænement
            retrain_interval_days: Intervalle minimum entre r√©-entra√Ænements (jours)
        """
        self.min_new_samples = min_new_samples
        self.retrain_interval_days = retrain_interval_days
        self.last_retrain_file = Path("backend/last_retrain_times.json")
        self.last_retrain_times = self._load_last_retrain_times()
    
    def _load_last_retrain_times(self) -> Dict[str, str]:
        """Charge les timestamps de dernier r√©-entra√Ænement par cat√©gorie"""
        if self.last_retrain_file.exists():
            try:
                with open(self.last_retrain_file, 'r') as f:
                    return json.load(f)
            except:
                return {}
        return {}
    
    def _save_last_retrain_time(self, category: str):
        """Sauvegarde le timestamp de dernier r√©-entra√Ænement"""
        self.last_retrain_times[category] = datetime.now().isoformat()
        with open(self.last_retrain_file, 'w') as f:
            json.dump(self.last_retrain_times, f, indent=2)
    
    def _should_retrain(self, category: str) -> bool:
        """V√©rifie si on doit r√©-entra√Æner pour cette cat√©gorie"""
        if category not in self.last_retrain_times:
            return True
        
        last_time = datetime.fromisoformat(self.last_retrain_times[category])
        days_since = (datetime.now() - last_time).days
        return days_since >= self.retrain_interval_days
    
    def load_predictions_for_category(self, category: str) -> Optional[pd.DataFrame]:
        """
        Charge toutes les pr√©dictions pour une cat√©gorie donn√©e
        """
        if not MT5_PREDICTIONS_DIR.exists():
            print(f"‚ùå Dossier Predictions non trouv√©: {MT5_PREDICTIONS_DIR}")
            return None
        
        all_predictions = []
        
        # Parcourir tous les fichiers CSV
        for csv_file in MT5_PREDICTIONS_DIR.glob("*_predictions.csv"):
            try:
                df = pd.read_csv(csv_file, sep=";", parse_dates=["time"])
                
                # Filtrer par cat√©gorie
                if "category" in df.columns:
                    df_cat = df[df["category"] == category]
                    if len(df_cat) > 0:
                        all_predictions.append(df_cat)
            except Exception as e:
                print(f"‚ö†Ô∏è Erreur lecture {csv_file.name}: {e}")
                continue
        
        if not all_predictions:
            return None
        
        combined = pd.concat(all_predictions, ignore_index=True)
        print(f"‚úÖ Charg√© {len(combined)} pr√©dictions pour cat√©gorie {category}")
        return combined
    
    def extract_features_from_predictions(self, predictions_df: pd.DataFrame) -> Optional[pd.DataFrame]:
        """
        Extrait les features depuis les pr√©dictions sauvegard√©es
        Note: Pour l'instant, on utilise les features stock√©es dans details_json
        √Ä am√©liorer: r√©cup√©rer les donn√©es OHLC r√©elles depuis MT5 pour recalculer les features
        """
        features_list = []
        
        for _, row in predictions_df.iterrows():
            try:
                details = json.loads(row["details_json"])
                ml_decision = details.get("ml_decision", {})
                
                # Si on a les features dans ml_decision, les utiliser
                if "input_row" in ml_decision:
                    features_list.append(ml_decision["input_row"])
            except Exception as e:
                continue
        
        if not features_list:
            return None
        
        features_df = pd.DataFrame(features_list)
        return features_df
    
    def create_labels_from_predictions(self, predictions_df: pd.DataFrame) -> Optional[pd.Series]:
        """
        Cr√©e les labels (targets) depuis les pr√©dictions
        Pour l'instant: action "buy" = 1, "sell" = 0, "hold" = exclu
        TODO: Am√©liorer avec les r√©sultats r√©els des trades (PnL r√©el)
        """
        # Filtrer seulement les d√©cisions de trading (exclure HOLD)
        trades = predictions_df[predictions_df["action"].isin(["buy", "sell"])].copy()
        
        if len(trades) == 0:
            return None
        
        # Label: 1 pour BUY, 0 pour SELL
        labels = (trades["action"] == "buy").astype(int)
        
        return labels
    
    def retrain_model_for_category(self, category: str, use_historical_data: bool = True) -> Dict:
        """
        R√©-entra√Æne le mod√®le pour une cat√©gorie donn√©e
        
        Args:
            category: Cat√©gorie √† r√©-entra√Æner (BOOM_CRASH, VOLATILITY, FOREX, etc.)
            use_historical_data: Si True, combine avec donn√©es historiques MT5 existantes
        """
        print(f"\nüîÑ R√©-entra√Ænement mod√®le pour cat√©gorie: {category}")
        
        # V√©rifier si on doit r√©-entra√Æner
        if not self._should_retrain(category):
            print(f"‚è∏Ô∏è  Trop t√¥t pour r√©-entra√Æner {category} (dernier: {self.last_retrain_times.get(category, 'jamais')})")
            return {"status": "skipped", "reason": "too_soon"}
        
        # Charger les pr√©dictions r√©centes
        predictions_df = self.load_predictions_for_category(category)
        if predictions_df is None or len(predictions_df) < self.min_new_samples:
            print(f"‚è∏Ô∏è  Pas assez de nouvelles donn√©es ({len(predictions_df) if predictions_df is not None else 0} < {self.min_new_samples})")
            return {"status": "skipped", "reason": "insufficient_data"}
        
        # Extraire features et labels
        features_df = self.extract_features_from_predictions(predictions_df)
        labels = self.create_labels_from_predictions(predictions_df)
        
        if features_df is None or labels is None or len(features_df) != len(labels):
            print(f"‚ùå Erreur extraction features/labels")
            return {"status": "error", "reason": "feature_extraction_failed"}
        
        # V√©rifier que toutes les features attendues sont pr√©sentes
        missing_features = set(EXPECTED_FEATURES) - set(features_df.columns)
        if missing_features:
            print(f"‚ö†Ô∏è  Features manquantes: {missing_features} - Remplissage avec 0")
            for feat in missing_features:
                features_df[feat] = 0
        
        # S√©lectionner seulement les features attendues
        X = features_df[EXPECTED_FEATURES].fillna(0)
        y = labels.values
        
        if len(X) < 50:  # Minimum absolu pour entra√Ænement
            print(f"‚ùå Pas assez d'√©chantillons apr√®s filtrage ({len(X)} < 50)")
            return {"status": "error", "reason": "insufficient_samples"}
        
        # Diviser train/test
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=0.2, random_state=42, stratify=y if len(np.unique(y)) > 1 else None
        )
        
        # Normaliser
        scaler = StandardScaler()
        X_train_scaled = scaler.fit_transform(X_train)
        X_test_scaled = scaler.transform(X_test)
        
        # Entra√Æner nouveau mod√®le XGBoost
        print(f"üìä Entra√Ænement sur {len(X_train)} √©chantillons...")
        model = xgb.XGBClassifier(
            n_estimators=200,
            max_depth=6,
            learning_rate=0.1,
            subsample=0.8,
            colsample_bytree=0.8,
            random_state=42,
            eval_metric='logloss'
        )
        
        model.fit(
            X_train_scaled, y_train,
            eval_set=[(X_test_scaled, y_test)],
            early_stopping_rounds=20,
            verbose=False
        )
        
        # √âvaluer
        y_pred = model.predict(X_test_scaled)
        y_pred_proba = model.predict_proba(X_test_scaled)[:, 1]
        
        accuracy = accuracy_score(y_test, y_pred)
        auc = roc_auc_score(y_test, y_pred_proba) if len(np.unique(y_test)) > 1 else 0.0
        
        print(f"‚úÖ Nouveau mod√®le - Accuracy: {accuracy:.3f}, AUC: {auc:.3f}")
        
        # Comparer avec ancien mod√®le si disponible
        config = MODEL_CONFIGS.get(category)
        if config and os.path.exists(config['model_path']):
            try:
                old_model = joblib.load(config['model_path'])
                old_scaler = joblib.load(config['scaler_path'])
                
                X_test_old_scaled = old_scaler.transform(X_test)
                old_pred = old_model.predict(X_test_old_scaled)
                old_accuracy = accuracy_score(y_test, old_pred)
                
                print(f"üìä Ancien mod√®le - Accuracy: {old_accuracy:.3f}")
                
                # Remplacer seulement si meilleur
                if accuracy > old_accuracy + 0.02:  # Au moins 2% d'am√©lioration
                    # Sauvegarder backup
                    backup_model = config['model_path'].replace('.pkl', '_backup.pkl')
                    backup_scaler = config['scaler_path'].replace('.pkl', '_backup.pkl')
                    os.rename(config['model_path'], backup_model)
                    os.rename(config['scaler_path'], backup_scaler)
                    
                    # Sauvegarder nouveau mod√®le
                    joblib.dump(model, config['model_path'])
                    joblib.dump(scaler, config['scaler_path'])
                    
                    print(f"‚úÖ Mod√®le remplac√© (am√©lioration: +{accuracy - old_accuracy:.3f})")
                    self._save_last_retrain_time(category)
                    
                    return {
                        "status": "success",
                        "category": category,
                        "old_accuracy": old_accuracy,
                        "new_accuracy": accuracy,
                        "improvement": accuracy - old_accuracy,
                        "samples_used": len(X)
                    }
                else:
                    print(f"‚è∏Ô∏è  Mod√®le non remplac√© (am√©lioration insuffisante: {accuracy - old_accuracy:.3f})")
                    return {
                        "status": "no_improvement",
                        "category": category,
                        "old_accuracy": old_accuracy,
                        "new_accuracy": accuracy,
                        "improvement": accuracy - old_accuracy
                    }
            except Exception as e:
                print(f"‚ö†Ô∏è  Erreur comparaison ancien mod√®le: {e}")
                # Sauvegarder quand m√™me le nouveau mod√®le
                joblib.dump(model, config['model_path'])
                joblib.dump(scaler, config['scaler_path'])
                self._save_last_retrain_time(category)
                return {"status": "success", "category": category, "accuracy": accuracy}
        else:
            # Pas d'ancien mod√®le, sauvegarder directement
            if config:
                joblib.dump(model, config['model_path'])
                joblib.dump(scaler, config['scaler_path'])
                self._save_last_retrain_time(category)
                return {"status": "success", "category": category, "accuracy": accuracy}
            else:
                return {"status": "error", "reason": "no_config"}
    
    def retrain_all_categories(self) -> Dict[str, Dict]:
        """R√©-entra√Æne tous les mod√®les qui ont assez de nouvelles donn√©es"""
        results = {}
        
        categories = ["BOOM_CRASH", "VOLATILITY", "FOREX", "COMMODITIES"]
        
        for category in categories:
            try:
                result = self.retrain_model_for_category(category)
                results[category] = result
            except Exception as e:
                print(f"‚ùå Erreur r√©-entra√Ænement {category}: {e}")
                results[category] = {"status": "error", "error": str(e)}
        
        return results


def main():
    """Point d'entr√©e pour r√©-entra√Ænement manuel"""
    import argparse
    
    parser = argparse.ArgumentParser(description="R√©-entra√Ænement continu des mod√®les ML")
    parser.add_argument("--category", help="Cat√©gorie sp√©cifique √† r√©-entra√Æner")
    parser.add_argument("--all", action="store_true", help="R√©-entra√Æner toutes les cat√©gories")
    parser.add_argument("--min-samples", type=int, default=100, help="Minimum d'√©chantillons requis")
    
    args = parser.parse_args()
    
    learner = ContinuousLearning(min_new_samples=args.min_samples)
    
    if args.all:
        results = learner.retrain_all_categories()
        print("\nüìä R√©sum√© r√©-entra√Ænement:")
        for cat, result in results.items():
            print(f"   {cat}: {result.get('status', 'unknown')}")
    elif args.category:
        result = learner.retrain_model_for_category(args.category)
        print(f"\nüìä R√©sultat: {result}")
    else:
        parser.print_help()


if __name__ == "__main__":
    main()

